{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saint Bernadette Soubirous']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\")\n",
    "squad = squad.flatten()\n",
    "squad[\"train\"][0][\"answers.text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import AutoTokenizer\n",
    "prefix_1 = \"answer: \"\n",
    "prefix_2 = \" context: \"\n",
    "prefix_3 = \"question: \"\n",
    "checkpoint = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "  input_1 = [ prefix_1 + i[0] for i in examples[\"answers.text\"] ]\n",
    "  input_2 = [ prefix_2 + i for i in examples[\"context\"] ]\n",
    "  input = [input_1[i] + input_2[i] for i in range(len(examples[\"context\"]))]\n",
    "  inputs = [input[0] for i in range(len(examples[\"context\"]))]\n",
    "  model_inputs = tokenizer(inputs, padding=\"longest\", max_length = max_source_length , truncation = True, return_tensors = \"pt\")\n",
    "  labels = tokenizer([prefix_3 + i for i in examples[\"question\"]], padding=\"longest\", max_length = max_target_length ,truncation = True, return_tensors = \"pt\")\n",
    "  labels[\"input_ids\"][labels[\"input_ids\"]==tokenizer.pad_token_id] = -100\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "prefix_1 = \"context: \"\n",
    "prefix_2 = \" question: \"\n",
    "prefix_3 = \"answer: \"\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "def preprocess_function(examples):\n",
    "  output_1 = [ prefix_2 + i for i in examples[\"question\"] ]\n",
    "  output_2 = [ prefix_3 + i[0] for i in examples[\"answers.text\"] ]\n",
    "  output = [output_2[i] + output_1[i] for i in range(len(examples[\"question\"]))]\n",
    "  assert len(examples[\"question\"]) == len(examples[\"answers.text\"])\n",
    "  model_inputs = tokenizer([prefix_1 + i for i in examples[\"context\"]], padding=\"max_length\", max_length = max_source_length , truncation = True, return_tensors = \"pt\")\n",
    "  labels = tokenizer(output, padding=\"max_length\", max_length = max_target_length, truncation = True, return_tensors = \"pt\")\n",
    "  labels[\"input_ids\"][labels[\"input_ids\"]==tokenizer.pad_token_id] = -100\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  model_inputs[\"target_mask\"] = labels[\"attention_mask\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_tokenized = squad.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_tokenized = squad_tokenized.remove_columns([\"id\",\"title\",\"question\",\"context\",\"answers.text\",\"answers.answer_start\"])\n",
    "squad_tokenized.set_format(\"torch\")\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'target_mask'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'target_mask'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1525,    10,  2788,  8942,     9,    26,  1954,   264,  8371,  8283,\n",
       "          822,    10,   304,  4068,   410,     8, 16823,  3790,     3, 18280,\n",
       "         2385,    16,   507,  3449,    16,   301,  1211,  1395,  1410,    58,\n",
       "            1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_tokenized[\"train\"][0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(squad_tokenized[\"train\"], batch_size=4, collate_fn=data_collator, num_workers=4)\n",
    "# eval_dataloader = DataLoader(squad_tokenized[\"validation\"], batch_size=4, collate_fn=data_collator, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = squad_tokenized[\"train\"]\n",
    "#squad_eval = squad_tokenized[\"validation\"]\n",
    "eval_test = squad_tokenized[\"validation\"].train_test_split(test_size=0.1, shuffle=False)\n",
    "squad_eval = eval_test[\"train\"]\n",
    "squad_test = eval_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = squad_train.select(range(10000))\n",
    "squad_eval = squad_eval.select(range(1000))\n",
    "squad_test = squad_test.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(squad_train, batch_size=4, shuffle=False, num_workers=4)\n",
    "eval_dataloader = DataLoader(squad_eval, batch_size=4, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(squad_test, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([4, 512]), 'attention_mask': torch.Size([4, 512]), 'labels': torch.Size([4, 128]), 'target_mask': torch.Size([4, 128])}\n",
      "context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "answer: a Marian place of prayer and reflection question: What is the Grotto at Notre Dame?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "print({k:v.shape for k,v in batch.items()})\n",
    "print(tokenizer.decode(batch[\"input_ids\"][3]))\n",
    "fake_labels = np.where(batch[\"labels\"][3]!=-100, batch[\"labels\"][3], tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(fake_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: Much of the work of the Scottish Parliament is done in committee. The role of committees is stronger in the Scottish Parliament than in other parliamentary systems, partly as a means of strengthening the role of backbenchers in their scrutiny of the government and partly to compensate for the fact that there is no revising chamber. The principal role of committees in the Scottish Parliament is to take evidence from witnesses, conduct inquiries and scrutinise legislation. Committee meetings take place on Tuesday, Wednesday and Thursday morning when Parliament is sitting. Committees can also meet at other locations throughout Scotland.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "answer: committee question: Where is much of the work of the Scottish Parliament done?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "  break\n",
    "print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "fake_labels = np.where(batch[\"labels\"][0]!=-100, batch[\"labels\"][0], tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(fake_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '572fc6f204bcaa1900d76cf6',\n",
       " 'title': 'Scottish_Parliament',\n",
       " 'context': 'Much of the work of the Scottish Parliament is done in committee. The role of committees is stronger in the Scottish Parliament than in other parliamentary systems, partly as a means of strengthening the role of backbenchers in their scrutiny of the government and partly to compensate for the fact that there is no revising chamber. The principal role of committees in the Scottish Parliament is to take evidence from witnesses, conduct inquiries and scrutinise legislation. Committee meetings take place on Tuesday, Wednesday and Thursday morning when Parliament is sitting. Committees can also meet at other locations throughout Scotland.',\n",
       " 'question': 'What are committees in the Scottish Parliament compared to other systems?',\n",
       " 'answers.text': ['stronger',\n",
       "  'stronger',\n",
       "  'stronger in the Scottish Parliament than in other parliamentary systems'],\n",
       " 'answers.answer_start': [92, 92, 92]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"validation\"][9514]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lionel Messi, who has been linked with a move to Manchester United, has been linked with a move to Barcelona.\\n\\nThe Argentina international has been linked with a move to Barcelona, with the club reportedly interested in the player.\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer(\"Lionel Messi,\",return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids = tokens.input_ids, attention_mask = tokens.attention_mask, max_length=50)\n",
    "tokenizer.decode(outputs[0])\n",
    "# softmax = torch.nn.Softmax(dim=1)\n",
    "# x = softmax(outputs.logits[0])\n",
    "# predictions = torch.argmax(x, dim=1)\n",
    "# decoded_preds = tokenizer.decode(predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9637, grad_fn=<NllLossBackward0>) torch.Size([4, 128, 32128])\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 32128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# softmax = torch.nn.Softmax(dim=2)\n",
    "# x = softmax(outputs.logits)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "decoded_preds = tokenizer.batch_decode(predictions)\n",
    "converted_labels = np.where(batch[\"labels\"]!=-100, batch[\"labels\"], tokenizer.pad_token_id)\n",
    "decoded_labels = tokenizer.batch_decode(converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer: committee question: Where is much of the work of the Scottish Parliament done?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'answer: stronger question: What are committees in the Scottish Parliament compared to other systems?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'answer: no revising chamber question: What is one avenue being compensated for by having committees serve such a large role?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " \"answer: principal role question: Taking evidence from witnesses is one of committees' what?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The questions nos</s></s> is the of the work of the Scottish Parliament done in</s></s> The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The back back The The The back back The back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back there back back back back back back back back back back back the the</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       " 'The questions no in than</s> is thes in the Scottish Parliament?? to in </s></s></s> The The The back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       " 'The questions no</s> ising chamber</s> : no is the committee of usedd for the the nos in as a purpose number?</s></s> The The The The The The The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       " 'The questions no role of of takeif evidence from witnesses, the of thes in primary matters</s></s> The the the the the the the the back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#   for batch in train_dataloader:\n",
    "#     batch = {k:v.to(device) for k,v in batch.items()}\n",
    "#     outputs = model(**batch)\n",
    "#     loss = outputs.loss\n",
    "#     loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "#     lr_scheduler.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"t5_question_generation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCQGenerator(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, model_name_or_path:str, \n",
    "               learning_rate:float=3e-4,\n",
    "              # batch_size:int=4,\n",
    "              #  experiment_name:str=\"learnX.ai (study support)\", \n",
    "              #  tracking_uri:str=\"http://127.0.0.1:8080\"\n",
    "              ):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "    self.learning_rate=learning_rate\n",
    "    # self.train_loss = []\n",
    "    # self.val_loss = []\n",
    "\n",
    "  def forward(self,**inputs):\n",
    "    return self.model(input_ids=inputs[\"input_ids\"], \n",
    "                      attention_mask=inputs[\"attention_mask\"], \n",
    "                      decoder_attention_mask=inputs[\"target_mask\"], \n",
    "                      labels=inputs[\"labels\"])\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    outputs = self(**batch)\n",
    "    loss = outputs[0]\n",
    "    #self.train_loss.append(loss)\n",
    "    self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "  \n",
    "  # def on_train_epoch_end(self):\n",
    "  #   loss = sum(self.train_loss)/len(self.train_loss)\n",
    "  #   self.logger.experiment.log_metric(run_id=self.logger.run_id, key=\"train_loss\", value=loss)\n",
    "  #   self.train_loss.clear()\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    outputs = self(**batch)\n",
    "    val_loss, logits = outputs[:2]\n",
    "    # self.val_loss.append(val_loss)\n",
    "    # preds = torch.argmax(logits, dim=2)\n",
    "    # labels = batch[\"labels\"]\n",
    "    # self.outputs[\"val_loss\"].append(val_loss)\n",
    "    # self.outputs[\"preds\"].append(preds)\n",
    "    # self.outputs[\"labels\"].append(labels)\n",
    "    self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return val_loss\n",
    "\n",
    "  # def on_validation_epoch_end(self):\n",
    "  #   loss = sum(self.val_loss)/len(self.val_loss)\n",
    "  #   self.logger.experiment.log_metric(run_id=self.logger.run_id, key=\"val_loss\", value=loss)\n",
    "  #   self.val_loss.clear()\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = AdamW(self.model.parameters(), lr=self.learning_rate, eps=1e-8)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params | Mode\n",
      "------------------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M | eval\n",
      "------------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "277       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [08:00<00:00,  5.20it/s, v_num=4a79, val_loss=1.850, train_loss=0.790]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [08:11<00:00,  5.09it/s, v_num=4a79, val_loss=1.850, train_loss=0.790]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/30 02:57:20 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run exultant-horse-117 at: http://127.0.0.1:8080/#/experiments/547642378050026970/runs/6067b533fc374c42bd0a2c04f69a4a79.\n",
      "2024/08/30 02:57:20 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/547642378050026970.\n"
     ]
    }
   ],
   "source": [
    "model = MCQGenerator(model_name_or_path=\"t5-small\")\n",
    "mlf_logger = MLFlowLogger(experiment_name=\"learnX.ai (study support)\", tracking_uri=\"http://127.0.0.1:8080\")\n",
    "# timer = Timer(duration=\"00:02:30:00\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"G:\\My Drive\\learnX.ai (study support)\\checkpoints\", filename=\"checkpoint-{epoch}-{step}\", save_top_k=1, save_last=True, every_n_epochs=3)\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator=\"gpu\", devices=\"auto\", logger=mlf_logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = MCQGenerator.load_from_checkpoint(\"G:\\My Drive\\learnX.ai (study support)\\checkpoints\\checkpoint-epoch=2-step=7500.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"learning_rate\":      0.0003\n",
       "\"model_name_or_path\": t5-small"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory ..\\547642378050026970\\122b6dc49c224959be300874a34506be\\checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at D:/learnx.ai/547642378050026970/122b6dc49c224959be300874a34506be/checkpoints/epoch=0-step=125.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name  | Type                       | Params | Mode\n",
      "------------------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M | eval\n",
      "------------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "277       Modules in eval mode\n",
      "Restored all states from the checkpoint at D:/learnx.ai/547642378050026970/122b6dc49c224959be300874a34506be/checkpoints/epoch=0-step=125.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/27 10:55:21 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run adorable-lamb-851 at: http://127.0.0.1:8080/#/experiments/547642378050026970/runs/122b6dc49c224959be300874a34506be.\n",
      "2024/08/27 10:55:21 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/547642378050026970.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, eval_dataloader, ckpt_path=\"G:\\My Drive\\learnX.ai (study support)\\checkpoints\\checkpoint-epoch=2-step=7500.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "state_dict = OrderedDict()\n",
    "\n",
    "for i,j in model_test.state_dict().items():\n",
    "    t = i.split(\".\",maxsplit=1)[1]\n",
    "    i = i.replace(i,t)\n",
    "    state_dict[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer: a copper statue of Christ question: What sits next to the Main Building in front of the Basilica of the Sacred Heart?',\n",
       " 'answer: the Grotto question: What is the name of the grotto at Lourdes, France?',\n",
       " 'answer: the Grotto question: What is the name of the grotto at Notre Dame?']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "preds = model.generate(input_ids = batch[\"input_ids\"][0].expand(1,-1), attention_mask = batch[\"attention_mask\"][0].expand(1,-1), max_length=200, num_beams=5, num_return_sequences=3)\n",
    "decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Rake, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "text = squad[\"train\"][\"context\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_nltk = Rake(max_length=3, \n",
    "                include_repeated_phrases = False)\n",
    "\n",
    "rake_nltk.extract_keywords_from_text(text)\n",
    "keywords = rake_nltk.get_ranked_phrases()\n",
    "\n",
    "filtered_keywords = set()\n",
    "#punctuation removal from the text followed\n",
    "for i in keywords:\n",
    "    i = i.translate(str.maketrans(\"\",\"\",string.punctuation)).strip()\n",
    "    filtered_keywords.add(i)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(filtered_keywords, 5)\n",
    "masked_sentence = []\n",
    "cased_keywords = ([(i.upper(), i.lower(), i.capitalize(), i.title()) for i in sample])\n",
    "temp = text\n",
    "\n",
    "for i,j,k,l in cased_keywords:\n",
    "  temp = temp.replace(i,\"[MASK]\").replace(j,\"[MASK]\").replace(k,\"[MASK]\").replace(l,\"[MASK]\")\n",
    "\n",
    "for i in nltk.sent_tokenize(temp):\n",
    "  if \"[MASK]\" in i:\n",
    "    masked_sentence.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Atop the Main Building's [MASK] is a golden statue of the Virgin Mary.\n",
      "1          Immediately behind the basilica is the Grotto, a [MASK] of [MASK] and reflection.\n",
      "2          It is a [MASK] of the grotto at [MASK], France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n",
      "3          At the end of the main drive (and in a direct line that connects through 3 statues and the [MASK]), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(masked_sentence):\n",
    "    print(\"{:}{:10}{:}\".format(i,\" \",j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keyword extraction using rake-nltk\n",
    "rake_nltk = Rake(max_length = 1, \n",
    "                include_repeated_phrases = False,\n",
    "                punctuations = string.punctuation)\n",
    "\n",
    "rake_nltk.extract_keywords_from_text(text)\n",
    "keywords = rake_nltk.get_ranked_phrases()[:10]\n",
    "\n",
    "#punctuation and stopwords removal from the text followed by lemmatization\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "text_no_punc = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "word_tokens = word_tokenize(text_no_punc.lower())\n",
    "#filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentence = [lemmatizer.lemmatize(w) for w in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_keywords = []\n",
    "wn_definitions = []\n",
    "try:\n",
    "    for i in keywords:\n",
    "        wn_keywords.append(i)\n",
    "        wn_definitions.append(lesk(lemmatized_sentence, i).definition())\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replica                                                                                             a person lacking intelligence or common sense\n",
      "mary                                                                                                the period of instruction in a school; the time period when school is in session\n",
      "next                                                                                                copy that is not the original; something that has been copied\n",
      "simple                                                                                              (mathematics) a transformation in which the direction of one axis is reversed\n",
      "prayer                                                                                              the act of communicating with a deity (especially as a petition or in adoration or contrition or thanksgiving)\n",
      "lourdes                                                                                             at the time or occasion immediately following\n",
      "reflection                                                                                          the mother of Jesus; Christians refer to her as the Virgin Mary; she is especially honored by Roman Catholics\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(wn_keywords, len(wn_keywords)-1)\n",
    "try:\n",
    "    for i in range(len(sample)):\n",
    "        print(\"{:100}{:10}\".format(sample[i], wn_definitions[i]))\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForPreTraining\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertForPreTraining.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\learnx.ai\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in d:\\learnx.ai\\.venv\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in d:\\learnx.ai\\.venv\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\learnx.ai\\.venv\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in d:\\learnx.ai\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\benepar\\parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<benepar.integrations.spacy_plugin.BeneparComponent at 0x1db31537a00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "import benepar\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.add_pipe(\"benepar\", config={\"model\":\"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (NP (NNP Kalki)) (, ,) (NP (NP (JJ final) (NN avatar)) (-LRB- -LRB-) (NP (NN incarnation)) (-RRB- -RRB-) (PP (IN of) (NP (NP (NP (DT the) (JJ Hindu) (NN god)) (NP (NNP Vishnu))) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBZ is) (ADVP (RB yet)) (S (VP (TO to) (VP (VB appear)))))))))))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\torch\\distributions\\distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Kalki, final avatar (incarnation) of the Hindu god Vishnu, who is yet to appear\")\n",
    "sent = list(doc.sents)[0]\n",
    "print(sent._.parse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NP',)\n"
     ]
    }
   ],
   "source": [
    "print(sent._.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((list(sent._.children)[1])._.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NP                                                                                                   \n",
      "   _________|_________________________                                                                            \n",
      "  |    |                              NP                                                                         \n",
      "  |    |          ____________________|______________________                                                     \n",
      "  |    |         |           |        |        |             PP                                                  \n",
      "  |    |         |           |        |        |     ________|___________________                                 \n",
      "  |    |         |           |        |        |    |                            NP                              \n",
      "  |    |         |           |        |        |    |              ______________|________                        \n",
      "  |    |         |           |        |        |    |             |              |       SBAR                    \n",
      "  |    |         |           |        |        |    |             |              |    ____|____                   \n",
      "  |    |         |           |        |        |    |             |              |   |         S                 \n",
      "  |    |         |           |        |        |    |             |              |   |         |                  \n",
      "  |    |         |           |        |        |    |             |              |   |         VP                \n",
      "  |    |         |           |        |        |    |             |              |   |     ____|________          \n",
      "  |    |         |           |        |        |    |             |              |   |    |    |        S        \n",
      "  |    |         |           |        |        |    |             |              |   |    |    |        |         \n",
      "  |    |         |           |        |        |    |             NP             |   |    |    |        VP       \n",
      "  |    |         |           |        |        |    |         ____|________      |   |    |    |     ___|____     \n",
      "  NP   |         NP          |        NP       |    |        NP            NP    |  WHNP  |   ADVP  |        VP  \n",
      "  |    |     ____|____       |        |        |    |    ____|________     |     |   |    |    |    |        |    \n",
      " NNP   ,    JJ        NN   -LRB-      NN     -RRB-  IN  DT   JJ       NN  NNP    ,   WP  VBZ   RB   TO       VB  \n",
      "  |    |    |         |      |        |        |    |   |    |        |    |     |   |    |    |    |        |    \n",
      "Kalki  ,  final     avatar -LRB- incarnation -RRB-  of the Hindu     god Vishnu  ,  who   is  yet   to     appear\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tree = Tree.fromstring(sent._.parse_string)\n",
    "print(tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NP \n",
      "  |   \n",
      " NNP \n",
      "  |   \n",
      "Kalki\n",
      "\n",
      " , \n",
      " |  \n",
      " , \n",
      "\n",
      "                            NP                                                                         \n",
      "        ____________________|______________________                                                     \n",
      "       |           |        |        |             PP                                                  \n",
      "       |           |        |        |     ________|___________________                                 \n",
      "       |           |        |        |    |                            NP                              \n",
      "       |           |        |        |    |              ______________|________                        \n",
      "       |           |        |        |    |             |              |       SBAR                    \n",
      "       |           |        |        |    |             |              |    ____|____                   \n",
      "       |           |        |        |    |             |              |   |         S                 \n",
      "       |           |        |        |    |             |              |   |         |                  \n",
      "       |           |        |        |    |             |              |   |         VP                \n",
      "       |           |        |        |    |             |              |   |     ____|________          \n",
      "       |           |        |        |    |             |              |   |    |    |        S        \n",
      "       |           |        |        |    |             |              |   |    |    |        |         \n",
      "       |           |        |        |    |             NP             |   |    |    |        VP       \n",
      "       |           |        |        |    |         ____|________      |   |    |    |     ___|____     \n",
      "       NP          |        NP       |    |        NP            NP    |  WHNP  |   ADVP  |        VP  \n",
      "   ____|____       |        |        |    |    ____|________     |     |   |    |    |    |        |    \n",
      "  JJ        NN   -LRB-      NN     -RRB-  IN  DT   JJ       NN  NNP    ,   WP  VBZ   RB   TO       VB  \n",
      "  |         |      |        |        |    |   |    |        |    |     |   |    |    |    |        |    \n",
      "final     avatar -LRB- incarnation -RRB-  of the Hindu     god Vishnu  ,  who   is  yet   to     appear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp1 = tree[0]\n",
    "temp2 = tree[1]\n",
    "temp3 = tree[-1]\n",
    "temp1.pretty_print()\n",
    "temp2.pretty_print()\n",
    "temp3.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split at rightmost NP or VP\n",
    "\n",
    "def get_flattened(t):\n",
    "    sent_str_final = None\n",
    "    if t is not None:\n",
    "        sent_str = [\" \".join(x.leaves()) for x in list(t)]\n",
    "        sent_str_final = [\" \".join(sent_str)]\n",
    "        sent_str_final = sent_str_final[0]\n",
    "    return sent_str_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rvp_nvp(parse_tree, last_np = None, last_vp = None):\n",
    "\n",
    "    if len(parse_tree.leaves()) == 1:\n",
    "        return last_np, last_vp\n",
    "    last_subtree = parse_tree[-1]\n",
    "    if last_subtree.label() == \"NP\":\n",
    "        last_np = last_subtree\n",
    "    elif last_subtree.label() == \"VP\":\n",
    "        last_vp = last_subtree\n",
    "    return get_rvp_nvp(last_subtree, last_np, last_vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Hindu god Vishnu , who is yet to appear\n",
      "appear\n"
     ]
    }
   ],
   "source": [
    "last_np, last_vp = get_rvp_nvp(tree)\n",
    "last_np_flattened = get_flattened(last_np)\n",
    "last_vp_flattened = get_flattened(last_vp)\n",
    "print(last_np_flattened)\n",
    "print(last_vp_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_termination_portion(main_string, sub_string):\n",
    "    combined_sub_string = sub_string.replace(\" \",\"\")\n",
    "    main_string_list = main_string.split()\n",
    "    last_index = len(main_string_list)\n",
    "    for i in range(last_index):\n",
    "        check_string_list = main_string_list[i:]\n",
    "        check_string = \"\".join(check_string_list)\n",
    "        check_string = check_string.replace(\" \",\"\")\n",
    "        if check_string == combined_sub_string:\n",
    "            return \" \".join(main_string_list[:i])\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Hindu god Vishnu , who is yet to appear\n"
     ]
    }
   ],
   "source": [
    "longest_phrase = max(last_np_flattened, last_vp_flattened)\n",
    "print(longest_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_phrase = re.sub(r\"-LRB-\", \"(\", longest_phrase)\n",
    "longest_phrase = re.sub(r\"-RRB-\", \")\", longest_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Hindu god Vishnu , who is yet to appear'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sentence = get_termination_portion(\"The old woman was sitting under a tree and sipping coffee\", longest_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "GPT2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "GPT2_model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=GPT2_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 464, 1468, 2415,  373, 5586,  739,  257, 5509,  290]])\n"
     ]
    }
   ],
   "source": [
    "partial_sentence = \"The old woman was sitting under a tree and\"\n",
    "input_ids = GPT2_tokenizer.encode(partial_sentence, return_tensors='pt')\n",
    "print(input_ids)\n",
    "maximum_length = len(partial_sentence.split())+40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "#Activate top_k sampling and top_p sampling with only from 90% most likely words\n",
    "\n",
    "sample_outputs = GPT2_model.generate(input_ids, do_sample=True, max_length=maximum_length, top_k=60, top_p=0.8, repetition_penalty=10.0, num_return_sequences=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old woman was sitting under a tree and there wasn't much in the way of foliage.\n",
      "The old woman was sitting under a tree and listening to her husband's sermon.\n",
      "The old woman was sitting under a tree and her mother stood behind the fence to keep it from falling out.\n",
      "The old woman was sitting under a tree and there, beside her husband on the edge of his bed.\n",
      "The old woman was sitting under a tree and she said something to the stranger, \"What is it?\"\n",
      "The old woman was sitting under a tree and one of the men in her way asked what he could do for you.\n",
      "The old woman was sitting under a tree and her husband in the back seat of his truck.\n",
      "The old woman was sitting under a tree and she kept on repeating the same word: \"This is my girl.\n",
      "The old woman was sitting under a tree and heard that the man who'd been there had come to fetch her some food.\n",
      "The old woman was sitting under a tree and waiting for him to come along.\n",
      "The old woman was sitting under a tree and listening to her thoughts.\n",
      "The old woman was sitting under a tree and it didn't seem as if she had the right idea.\n"
     ]
    }
   ],
   "source": [
    "generated_sentences = []\n",
    "for i,sample_output in enumerate(sample_outputs):\n",
    "  decoded_sentence = GPT2_tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "  final_sentence = tokenize.sent_tokenize(decoded_sentence)[0]\n",
    "  generated_sentences.append(final_sentence)\n",
    "  print(final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learnx.ai\\.venv\\lib\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Architecturally, the school has a \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Catholic\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " character. Atop \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Main Building's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " gold dome is a golden statue of \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Virgin Mary.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " Immediately in front of \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Main Building\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " and facing it, is a copper statue of Christ with arms upraised with the legend &quot;\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Venite Ad Me Omnes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       "&quot;. Next to \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Main Building\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " is the Basilica of \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Sacred Heart\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       ". Immediately behind the basilica is the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Grotto\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Marian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " place of prayer and reflection. It is a replica of the grotto at \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lourdes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    France\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " where \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Virgin Mary\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " reputedly appeared to \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Saint Bernadette Soubirous\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1858\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". At the end of the main drive (and in a direct line that connects through \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " statues and \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Gold Dome\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "), is a simple, modern stone statue of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mary\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(squad[\"train\"][\"context\"][0])\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
