# learnX.ai
## Introduction
LearnX.ai aims to solve the problem for non-learners by providing the customized learning approach where user can input text for the things he/she wants to learn and it will generate the questions for them so that they can learn dynamically.
## Goals
1. To help instructors and students to personalize their learning methods.
2. To generate high quality questions that would help students to crack their tests and exams.
3. To instantly get the highly engaging materials.
## Contributors
The contributor of this project is Amir Bhattarai.
[www.linkedin.com/in/amir-bhattarai-1170511ab]
## Project Architecture
![Encoder (2)](https://github.com/user-attachments/assets/84541694-2185-4481-af6f-9fbe3209c347)
# Status
The literature review of the project is thoroughy done and no more in-depth study is required for the completion. The training phase of the transformers are carried out iteratively. The development of user interface is also done. A minimalist system is prepared.
## Known Issue
1. One known issue of the project is that the small dataset and minimal architecture has to led to slight underfitting while generating multiple choice questions.
2. Questions where answers are made up of multiple words and whose part of speech is ambiguous either distractors are not found or distractors are not appropriate.
3. Sometimes the answer generated by the pretrained transformer for question answering is not correct.
4. True/False Generator does not work as expected when given more than one sentences.
## High Level Next Steps
1. To carry out few necessary enhancements in question generation process.
2. To create a production level environment where users can interact with the system.
# Usage
## Installation
To begin this project, use the included `Makefile`

#### Creating Virtual Environment

This package is built using `python-3.8`. 
We recommend creating a virtual environment and using a matching version to ensure compatibility.

#### pre-commit

`pre-commit` will automatically format and lint your code. You can install using this by using
`make use-pre-commit`. It will take effect on your next `git commit`

#### pip-tools

The method of managing dependencies in this package is using `pip-tools`. To begin, run `make use-pip-tools` to install. 

Then when adding a new package requirement, update the `requirements.in` file with 
the package name. You can include a specific version if desired but it is not necessary. 

To install and use the new dependency you can run `make deps-install` or equivalently `make`

If you have other packages installed in the environment that are no longer needed, you can you `make deps-sync` to ensure that your current development environment matches the `requirements` files. 

## Usage Instructions
After the sucessful installation, run both streamlit and uvicorn servers in localhost with different ports. Open the streamlit frontend on your favourite browser. In the homepage, you can see the sidebar where you can see the multiple options such as Multiple Choice Questions, Fill in the blanks, Match the following, True/False and Flashcards. Choose what you want to generate. As soon as you click the button a new page appears. All the page has the same layout. Input context is provided on the text input area. Click on generate button and wait until you get the response.

# Data Source
The data is available at huggingface datasets and can be accessed using the link provided below.
[https://huggingface.co/datasets/rajpurkar/squad]
## Code Structure
The code for the project resides on the directory named "src". Inside the "src" directory there is another directory named named "learnx.ai" which contains the functional part of this project. Inside this directory there are numerous files and a folder named "pages". The name pages stores the webpages for the streamlit frontend. The FastAPI backend code is hosted on the file "app.py". The config.toml is the configuration file for the streamlit for storing themes. The "Homepage.py" is the main entry point for our streamlit application. The other files such as "Mcq.py", "QAGenerator.py", "TokenClassification.py" and "True_False.py" stores the functions and classes for data pre-processing, training the models and post-processing.
## Artifacts Location
The artifacts are stored in the drive link given below. Inside this there is a folder named checkpoints which contains the last checkpoint when training ends. There remain files which are named last-t5b.ckpt, last-t5s.ckpt which corresponds to the t5-base model and t5-small model fine-tuned for question and/or generation tasks respectively. Similarly, the last-tc.ckpt corresponds to the distilbert-base-uncased model fine-tuned for token classification. There also remain two other folders named squad and squad-iob respectively. The squad folder contains the tokenized version of already available squad dataset customized for question and/or generation task. likewise, the squad-iob is prepared for token classification but not tokenized.
[https://drive.google.com/drive/folders/1ZsNwsvuFRQwiP1N0hlUJ4pP6PmuKw2C9?dmr=1&ec=wgc-drive-hero-goto]
# Results
Overall, the system performance shows that it needs some improvement in question generation and distractors generation. A more powerful metric such as "rquge" is required to correctly assess the performance of Q/A generation model. The token classification model has no major failure so can be very effective. Q/A flashcards generation is also in acceptable state. True/False uses GPT-2 sentence completion which further needs some improvement such as to incorportate semantic textual similarity.
## Metrics Used
For the Q/A task, we used the "rouge" metric which didnot proved to be effective. However, it is mostly used for summarization tasks. Due to some limitations on hardware we could not use "rquge" metric. For the Token Classification task, we used the "f1-score", "precision", "recall" and accuracy.
## Evaluation Results
The rouge metric in question generation task shows that the metric is not a good measure of finding how good a generated question is because the value of rougeLsum is 0.09577 at the end of 3rd epoch, while for the token classification the accuracy 0.9598 at the end of 3rd epoch shows that the token classification is improving. The response time of the request is found to be 13 seconds on average. The latency in the system is mostly due to distractors generation code.
